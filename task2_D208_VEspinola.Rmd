---
title: "Odds of Churn"
author: "Victoria Espinola"
date: "2022-09-28"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE, warning= FALSE}
knitr::opts_chunk$set(
    echo        = TRUE,
    message     = F,
    warning     = F,
    paged.print = FALSE, 
    # This should allow Rmarkdown to locate the data
    root.dir    = rprojroot::find_rstudio_root_file()
)
```

```{r}
#Packages used for analysis and source code for data prep. 

library(tidyverse)
library(broom)
library(PerformanceAnalytics)
library(caret)
library(MASS)
library(pROC)
source("cleaner_copy.R")
```

#### Part 1: Research Question
###### A1. This project will focus on understanding the odds that a customer will churn based on customer profile comprised of tenure, usage, and survey responses. 

###### A2. Since new customer aquisition is 10 times that of keeping an existing customer, this question is fitting for stakeholders to understand customer life cycles and key metrics for improving customer experience by including customer survey responses and bandwidth consumption. 

###### B1. The assumptions of the logistic regression model are: (1) the response variable is binary, (2) the observations are independent of each other, (3) There is no multicollinearity between explanatory variables, (4) there are no extreme outliers in the data set, (5) there is a linear relationship between the response and explanatory variables, and (6) the sample size is suffiently large (statology). 

###### B2. The benefits for using R is that R is a statistical programming language and inherently has many statistical applications build into it. According to Srinivasan from the Pluralsight video, Understanding and Applying Logistic Regression, R is the best tool to use for regression. (Logistic Regression). 

###### B3. Logistic regression is an appropriate technique for this anaylsis because the response variable is binary, i.e., a customer only has two outcomes in behavior, they either churn, discontinue services or do not churn and retain services. 

##### Part III. Data Preparation
###### C1.Upon bringing in the data, the cleaning function script was used, this eliminates outlier variables and removes all missing values by imputing them with the mean and removing outliers from the data set. The preparation goals are to scale the observations for each variable by using log10 + 1 method. This will help to ensure the outcomes are not being heavily influenced by large quantities. The goal of this manipulation is to improve the understanding of the logistic model and will be reversed before the final model is interpretted.

```{r}
data <- read.csv("clean_data.csv", header = TRUE) %>% janitor::clean_names()
```


###### C2. The initial variable selection was based on key variables that are influenced by stakeholder actions, thus customer identification variables, marital status, age, children, gender, and location variables were all dropped before any analysis was completed. Additionally, this seemed an appropriate step to convert churn to 1, 0 values to ready the data for an ANOVA test. 

```{r}
data_no_demo <- data %>% mutate(churn = as.numeric(ifelse(data$churn == "Yes", 1, 0))) %>% select(churn:item8)
```

###### The variables were further reduced based on the p-values from results of the ANOVA calculation. The p-values that went into the reduced date set are all statistically significant with p-values less than 0.05. 

```{r}
aov <- aov(formula = churn ~ . , data = data_no_demo)
summary(aov)
```

###### Using the p-value and the potential for influence by stakeholders the following variables from the ANOVA test results have been selected to be used in the model: email, techie, contract, internet_service, phone, multiple, online_backup, online_protection, tech_support, streaming_tv, streaming_movies, payment_method, tenure, monthly_charge 
```{r}
#Summary Statistics 
skimr::skim(data_no_demo)
```

###### As observed in the summary statistic tables, appropriate scaling will be needed to avoid unnessisary influence from varying data scales, for instance, monthly charge values are dramatically different from emails exchanged between customers and their telecommunication providers. The logic behind using log10 + 1 is to change the scale and avoid creating any NaN values or missing observations. This method was applied to all numeric data with the exception of churn, therefore not factor variables were effected by this change. It is also noteworthy that compared to the mean, monthly charge and bandwidth have the most variation in their observations. 


###### C3. Below is the code with annotations showing the data preparation explained in C1 and will take into consideration the observations that were made the summary statistic from C2. First, the data used for preparation does not include demographic informations including case order, uid, state, county, zip, lat, lng, area, time zone, children, age, income, marital or gender using previously observed analysis these variables do not have an impact on predicting churn and they are not influenced by telecommunication stakeholders, thus they have been eliminated from the analysis. Additionally, the churn variable has already been changed to a numeric representation using 0's and 1's.  
```{r}
#Data selected omitted demographic information and has already had the cleaning script applied to it which imputes all variable with missing observations using either the mean or median and outliers in the data set greater than 3 standard deviations were removed.  
data_selected_scaled <- data_no_demo %>% #this data set has not demographic information 
                select(churn, email, techie, contract, internet_service, phone, multiple, online_backup,
                       device_protection, tech_support, streaming_tv, streaming_movies, payment_method, 
                       tenure, monthly_charge)%>% #these are the selected variables as identified by the ANOVA test
                mutate(across(where(is.numeric) &!c(churn), log1p)) #applying log10 + 1 to all numeric variables. 
```
###### Finally, the catagorical data was dummied and was then joined back together column wise, then all the duplicate columns were removed. 
```{r}
one_hot_dummy <- dummyVars(" ~.", data = (data_selected_scaled %>% select_if(is.factor))) #creates a full list of variable to be dummied, these factor variables are items with more than one level. 
dummy_df <- data.frame(predict(one_hot_dummy, newdata = (data_selected_scaled %>% select_if(is.factor)))) #here a new data frame is created for ease of rejoining to the numerically prepared data set. 

final_data_set <- dummy_df %>% bind_cols(data_selected_scaled) %>% #put together numeric with dummied variables
                  select(-(email:payment_method))%>%  #remove all non-dummied variables 
                  janitor::clean_names() #standardize names again after dummy process. 
```
###### C4. The univariate and bivariate graphs are shown below. The univariate graphs provide a variable level view of each distribution. The numeric visualizations are seen in the box plot and the factor variables are seen in the bar charts that follow. 
```{r}
boxplot(data_selected_scaled %>% select_if(is.numeric))
```
```{r, warning=FALSE}
factor_var <- data_selected_scaled %>% select_if(is.factor) %>% colnames() %>% as.list()

for (i in 1:length(factor_var)) {
  f <- ggplot(data = data_selected_scaled, aes(data_selected_scaled[[i]])) +
    geom_bar()+
    xlab( factor_var[[i]])
  print(f)
}
```

```{r, warning=FALSE, message=FALSE}
#C4- Bivariate plots using ggpairs (R-Bloggers)
ggpairs(data_selected_scaled, columns = c("techie", "contract", "internet_service", "phone", "multiple", "online_backup", "device_protection", "tech_support", "streaming_tv", "streaming_movies", "payment_method", "tenure", "monthly_charge"), title = "Bivariate Plot of Explanatory Variables with Churn indicator", mapping = aes(color = data$churn))
```
###### C5. The final data set will be attached in the submission. The code below was used to generate the CSV. 
```{r}
#C5 generating the prepared data set as csv. 
write.csv(final_data_set, "final_data_set.csv")
```

##### Part IV: Model Comparison and Anaylysis 

###### D1. The initial model includes all the variables identified in C2 from the final data set. This summary, coefficients, and AIC values are given after the code. 
```{r, echo = TRUE, warning=FALSE}
logit_1 <- glm(churn ~ ., data = final_data_set, family = "binomial") 
summary(logit_1)
coef(logit_1)
print(logit_1$aic) 
```

###### D2. The initial model contains many values with statisticall insignificant p-values and has an AIC value of 4411.38. Since a full model was created, a backward selection method will be used to find the final model. 

###### D3. The reduced regression model is given in the code below. 
```{r}
step_model <- logit_1 %>% stepAIC (trace = FALSE, direction = "backward")
coef(step_model)
print(step_model$aic)
```
 
###### E. The analysis began with an inquiry on customer churn odds based largely on stakeholders ability to influence customer outcomes; the variables selected were based on the ANOVA test p-values. Then starting with a cleaned and prepared data set all varibales were used in the inital model. Starting with the full model, a backward selection process was used to find the final model. Based on the similar AIC values of the full model, 4411.38 and the reduced model, 4405.51, the reduced model is preferred due to its simplicity. To further determine the performance of each model the ROC curve and AUC values will used in evaluating the performance metric as seen in the code below. 

```{r, echo = TRUE, warning=FALSE}
#ROC curves and AUC for comparison 
roc(final_data_set$churn, logit_1$fitted.values, plot = TRUE, legacy.axes = TRUE, percent = TRUE, 
xlab = "False Positive Percent", ylab = "True Positive Percent", print.auc = TRUE,print.auc.y = 95)
plot.roc(final_data_set$churn, step_model$fitted.values, percent = TRUE, col = "blue", print.auc = TRUE, add = TRUE, print.auc.y = 85)
```
###### Using the ROC curve and AUC values shown in the graph above, the initial and final model have the same ROC cuve and AUC value. With both having a relatively high AUC value, both models do a good job of predicting churn rates. Therefore, since the step model has fewer variables it will be the final model. 

###### E2. The follow code was used to produce the confusion matrix and its visualization will follow. 
```{r, echo = TRUE, warning=FALSE}
pred_data <-  data %>%
  mutate(
    has_churned = predict(step_model, final_data_set, type = "response")
  )

actual <- final_data_set$churn
predicted <- round(fitted(step_model))

outcomes <- table(predicted, actual)
outcomes

confusion <- yardstick::conf_mat(outcomes)
autoplot(confusion, )
```

##### Part V. Data Summary and Implications

###### F1. The regression equation for the final model that includes the response variable, churn, and explanatory variables, bandwidth_gb_year, tenure, monthly_charge, and techie is y =
```{r, echo= TRUE, warning=FALSE}
#F1
step_coeffs <- coef(step_model)
coeffs <- exp(coef(step_model)) - 1
step_coeffs
coeffs
reg_eq = -0.999 + -.588*final_data_set$techie_no + 21.323*final_data_set$contract_month_to_month + 1.67*final_data_set$internet_service_dsl +
        -0.51*final_data_set$internet_service_fiber_optic + 0.405*final_data_set$phone_no + -0.542*final_data_set$multiple_no + -0.211*final_data_set$online_backup_no +          -0.799*final_data_set$streaming_tv_no + -0.843*final_data_set$streaming_movies_no +-0.143*final_data_set$payment_method_bank_transfer_automatic 
        +0.372*final_data_set$payment_method_electronic_check +-0.889*final_data_set$tenure+71.772*final_data_set$monthly_charge
```
###### F1. According to the final model, .... However, using a combination of variable elimination from the final model did not provide a better ROC curve or AUC value, so all variables were kept in the final model due to their importance value and p-value. Most importantly, the model was limited in the scope of the question, as the variables chosen were included or excluded based on their potenial for stakeholder influence.  

###### F2. 

##### H. Sources
###### 1. The 6 Assumptions of Logistic Regression (With Examples). Zach. https://www.statology.org/assumptions-of-logistic-regression/

###### 2. Understanding and Applying Linear Regression. https://app.pluralsight.com/player?course=understanding-applying-linear-regression&author=vitthal-srinivasan&name=understanding-applying-linear-regression-m0&clip=0&mode=live

###### 3. ggpairs in R- A Brief Introduction to ggpairs: R-bloggers. Finnstats. https://www.r-bloggers.com/2021/06/ggpairs-in-r-a-brief-introduction-to-ggpairs/

###### 4. DataCamp. D208. https://app.datacamp.com/learn





